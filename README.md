# [Code for paper "Towards More Robust Retrieval-Augmented Generation: Evaluating RAG Under Adversarial Poisoning Attacks](https://arxiv.org/pdf/2412.16708)"


## Data
Please download the data on google drive [here](https://drive.google.com/drive/folders/1LwKIGTAY2V-xMRPmmkxZR1lRaMkpGlk8?usp=drive_link). 



Data can be find in ```results/adv_and_guiding_contexts``` directory, where ```guiding_contexts``` are generated by prompting gpt-4; while ```adv_contexts``` are from the original [PoisonedRAG paper](https://github.com/sleeepeer/PoisonedRAG). 

The ```results/pre-processed``` is the preprocessed data containing query and the top-10 untouched context, 5 adv context, and 5 guiding context, ranked by their similarity scores according to different retrievers. 

Final RAG outputs from LLMs will be in ```results/LLM_output_results``` directory.



## Code for producing the pre-processed data
```
python preprocess.py --eval_model_code 'ance' --dataset_name 'hotpotqa'
# choose eval_model_code among ['dpr-multi', 'dpr-single', 'contriever', 'ance', 'contriever-msmarco']
# choose dataset_name among ['hotpotqa', 'nq']
```

## Code for multiple context experiments (Dilution, pollution rate and counteract)
```
python MixedContext.py --model_name 'gpt3.5' \
# choose from ['gpt3.5', 'gpt4', 'gpt4o', 'claude', 'llama8b', 'llama70b']
--prompt_type 'skeptical' \
# choose from ['skeptical', 'faithful', 'neutral']
--dataset_name 'nq' \
# choose from ['hotpotqa', 'msmarco', 'nq']

```

# Code for using generation and retrieval together (top-10 retrieval)
```
python top10.py
```


